# -*- coding: utf-8 -*-
"""Trial-1-KG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19v4QynRoBTtqWlr6MF915wYbAAV370FJ
"""

!pip install networkx pyvis

"""Random small graph"""

import networkx as nx
from pyvis.network import Network

def create_pe_knowledge_graph():
    # Initialize a directed graph
    G = nx.DiGraph()

    # 1. Define Entities (Nodes)
    # Categories: Symptoms, Risk Factors, Tests, Diagnoses
    nodes = [
        ("Dyspnea", {"type": "Symptom"}),
        ("Pleuritic Chest Pain", {"type": "Symptom"}),
        ("Tachycardia", {"type": "Symptom/Sign"}),
        ("Hemoptysis", {"type": "Symptom"}),
        ("DVT_Symptoms", {"type": "Sign", "label": "Clinical Signs of DVT"}),
        ("Wells_Score", {"type": "Decision_Tool"}),
        ("D_Dimer", {"type": "Lab_Test"}),
        ("CTPA", {"type": "Imaging"}),
        ("V/Q_Scan", {"type": "Imaging"}),
        ("Pulmonary_Embolism", {"type": "Diagnosis"}),
        ("Alternative_Diagnosis", {"type": "Diagnosis"})
    ]

    for node, attrs in nodes:
        G.add_node(node, **attrs)

    # 2. Define Relationships (Edges)
    # Format: (Source, Target, Relationship_Type)
    relationships = [
        ("Dyspnea", "Wells_Score", "contributes_to"),
        ("Tachycardia", "Wells_Score", "contributes_to"),
        ("DVT_Symptoms", "Wells_Score", "high_weight_contribution"),
        ("Wells_Score", "D_Dimer", "indicates_test_if_low_risk"),
        ("Wells_Score", "CTPA", "indicates_test_if_high_risk"),
        ("D_Dimer", "Pulmonary_Embolism", "rules_out_if_negative"),
        ("CTPA", "Pulmonary_Embolism", "confirms_or_excludes"),
        ("V/Q_Scan", "Pulmonary_Embolism", "alternative_imaging")
    ]

    for src, dst, rel in relationships:
        G.add_edge(src, dst, relation=rel)

    return G

def visualize_graph(G):
    net = Network(notebook=True, directed=True, height="500px", width="100%")
    net.from_nx(G)

    # Customize colors based on node type
    color_map = {
        "Symptom": "#ffcccb",
        "Lab_Test": "#add8e6",
        "Imaging": "#90ee90",
        "Decision_Tool": "#f0e68c",
        "Diagnosis": "#ffa07a"
    }

    for node in net.nodes:
        node_type = G.nodes[node['id']].get('type', 'Unknown')
        node['color'] = color_map.get(node_type, "#eeeeee")
        node['title'] = f"Type: {node_type}"

    net.show("pe_diagnosis_kg.html")

# Execute
if __name__ == "__main__":
    pe_kg = create_pe_knowledge_graph()
    visualize_graph(pe_kg)
    print("Graph constructed. Open 'pe_diagnosis_kg.html' to view.")

# Update fundamental build tools
!pip install -U pip setuptools wheel

# Install the primary frameworks
!pip install spacy scispacy networkx pandas

# Download the small biomedical model directly via URL (most reliable)
!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz

!pip install -U -q "google-genai"

# IMPORTANT: Due to potential numpy version conflicts with other pre-installed libraries in Colab,
# it's highly recommended to restart the Colab runtime (Runtime -> Restart runtime)
# after this cell finishes executing, and then re-run the notebook.

import pandas as pd
import spacy
import scispacy
from scispacy.abbreviation import AbbreviationDetector
from scispacy.linking import EntityLinker
import networkx as nx

# 1. Setup the Clinical NLP Pipeline
nlp = spacy.load("en_core_sci_sm")
nlp.add_pipe("abbreviation_detector")
# We resolve abbreviations before linking for 15%+ better accuracy
nlp.add_pipe("scispacy_linker", config={"resolve_abbreviations": True, "linker_name": "umls"})

def process_impressions_pilot(file_path, limit=20):
    # Load your uploaded file
    df = pd.read_csv(file_path, sep='\t').head(limit)

    # Initialize the Knowledge Graph
    G = nx.MultiDiGraph()
    linker = nlp.get_pipe("scispacy_linker")

    for idx, row in df.iterrows():
        text = row['impressions']
        doc = nlp(text)

        # 2. Extract Entities and Link to UMLS (SNOMED-backed)
        entities = []
        for ent in doc.ents:
            # Get top UMLS CUI candidate
            cui = ent._.kb_ents[0][0] if ent._.kb_ents else "Unknown"
            entities.append({"text": ent.text, "cui": cui})

        # 3. MOCK LLM: Relationship Extraction
        # In the next step, you will replace this with your LLM API call
        # It will use the entities identified above to form triples
        triples = [
            (entities[0]['text'], "manifested_as", entities[1]['text']) if len(entities) > 1 else None
        ]

        # 4. Add to Graph
        for t in triples:
            if t:
                G.add_edge(t[0], t[2], relation=t[1], sample_id=row['impression_id'])

    return G

# Execute Pilot
kg_pilot = process_impressions_pilot('impressions_20250611.tsv')
print(f"Pilot KG Built: {kg_pilot.number_of_nodes()} nodes, {kg_pilot.number_of_edges()} edges.")

from pyvis.network import Network
import os

def visualize_kg(nx_graph, output_file="pe_pilot_graph.html"):
    # 1. Initialize Pyvis Network
    # Use notebook=True if you are in Jupyter/Kaggle
    net = Network(height="750px", width="100%", bgcolor="#222222", font_color="white", directed=True)

    # 2. Import the NetworkX graph
    net.from_nx(nx_graph)

    # 3. Add Physics Buttons (to untangle the nodes)
    net.show_buttons(filter_=['physics'])

    # 4. Save and open
    net.write_html(output_file)
    print(f"âœ… Graph saved to {os.path.abspath(output_file)}. Open this file in your browser.")

# Usage
visualize_kg(kg_pilot)

"""# KG from sample data - (scispacy)"""

import pandas as pd
from google import genai
from google.genai import types
import spacy
import scispacy
from scispacy.abbreviation import AbbreviationDetector
from scispacy.linking import EntityLinker
import networkx as nx
import json
import time

# --- MIGRATED SETUP ---
# The new SDK uses a Client object instead of genai.configure()
client = genai.Client(api_key="AIzaSyAM84bqxaWA58zygghqrzO0FAHz00x5fzw")

# Load ScispaCy with Entity Linker for Grounding
nlp = spacy.load("en_core_sci_sm")
nlp.add_pipe("abbreviation_detector")
nlp.add_pipe("scispacy_linker", config={"resolve_abbreviations": True, "linker_name": "umls"})
linker = nlp.get_pipe("scispacy_linker")

def get_grounded_entity(text):
    """Standardize string to UMLS CUI."""
    doc = nlp(text)
    if doc.ents and doc.ents[0]._.kb_ents:
        cui = doc.ents[0]._.kb_ents[0][0]
        return cui, linker.kb.cui_to_entity[cui].canonical_name
    return "Unknown", text

def run_migrated_pilot(file_path, limit=20):
    df = pd.read_csv(file_path, sep='\t').head(limit)
    G = nx.MultiDiGraph()

    # Define System Instructions as a variable (New SDK Preference)
    sys_instruct = (
        "Extract clinical triplets for a PE diagnosis KG. "
        "Nodes: Anatomy, Pathology, Assertion. Relations: located_in, manifested_as, associated_with."
    )

    for _, row in df.iterrows():
        try:
            print(f"Processing {row['impression_id']}...")

            # MIGRATED CALL: Use client.models.generate_content
            response = client.models.generate_content(
                model="gemini-2.0-flash", # Updated to latest stable version
                contents=row['impressions'],
                config=types.GenerateContentConfig(
                    system_instruction=sys_instruct,
                    response_mime_type="application/json",
                    # You can also use a Pydantic model here for even stricter typing
                    response_schema={
                        "type": "OBJECT",
                        "properties": {
                            "triplets": {
                                "type": "ARRAY",
                                "items": {
                                    "type": "OBJECT",
                                    "properties": {
                                        "subject": {"type": "STRING"},
                                        "predicate": {"type": "STRING"},
                                        "object": {"type": "STRING"}
                                    }
                                }
                            }
                        }
                    }
                )
            )

            # The response is now a pydantic-like object; access text directly
            data = json.loads(response.text)

            for tri in data.get('triplets', []):
                subj_cui, subj_name = get_grounded_entity(tri['subject'])
                obj_cui, obj_name = get_grounded_entity(tri['object'])

                G.add_edge(
                    subj_name, obj_name,
                    relation=tri['predicate'],
                    cui_pair=(subj_cui, obj_cui)
                )

            time.sleep(4) # Rate limit safety for free tier

        except Exception as e:
            print(f"Error: {e}")

    return G

# --- EXECUTION ---
kg_pilot = run_migrated_pilot('impressions_20250611.tsv', limit=20)

import pandas as pd
from google import genai
from google.genai import types
import spacy
import scispacy
from scispacy.abbreviation import AbbreviationDetector
from scispacy.linking import EntityLinker
import networkx as nx
import json
import time

# --- INITIALIZATION ---
client = genai.Client(api_key="AIzaSyCP7Xe0-bQ9Fg9-wcpfpDqdCv6fL6UIm5s")

# Load ScispaCy for local normalization and linking
nlp = spacy.load("en_core_sci_sm")
nlp.add_pipe("abbreviation_detector")
nlp.add_pipe("scispacy_linker", config={"resolve_abbreviations": True, "linker_name": "umls"})
linker = nlp.get_pipe("scispacy_linker")

def get_grounded_node(text):
    """Link text to UMLS CUI to merge synonymous nodes."""
    doc = nlp(text)
    if doc.ents and doc.ents[0]._.kb_ents:
        cui = doc.ents[0]._.kb_ents[0][0]
        return linker.kb.cui_to_entity[cui].canonical_name, cui
    return text, "Unknown"

def extract_with_retry(content, sys_instruct, max_retries=5):
    """Handles 429 errors by waiting and retrying."""
    delay = 30  # Initial wait for free tier exhaustion
    for attempt in range(max_retries):
        try:
            response = client.models.generate_content(
                model="gemini-1.5-flash",
                contents=content,
                config=types.GenerateContentConfig(
                    system_instruction=sys_instruct,
                    response_mime_type="application/json",
                    response_schema={
                        "type": "OBJECT",
                        "properties": {
                            "triplets": {
                                "type": "ARRAY",
                                "items": {
                                    "type": "OBJECT",
                                    "properties": {
                                        "subject": {"type": "STRING"},
                                        "predicate": {"type": "STRING"},
                                        "object": {"type": "STRING"}
                                    }
                                }
                            }
                        }
                    }
                )
            )
            return json.loads(response.text)
        except Exception as e:
            if "429" in str(e):
                print(f"âš ï¸ Quota hit. Retrying in {delay}s... (Attempt {attempt+1}/{max_retries})")
                time.sleep(delay)
                delay *= 2  # Exponential backoff
            else:
                raise e
    return None

# --- MAIN EXECUTION LOOP ---
def run_final_pilot(file_path, limit=100):
    df = pd.read_csv(file_path, sep='\t').head(limit)
    G = nx.MultiDiGraph()
    sys_instruct = "Extract clinical triplets: Anatomy, Pathology, Assertion."

    for _, row in df.iterrows():
        print(f"Processing ID: {row['impression_id']}")

        # 1. Local Abbreviation Expansion
        doc = nlp(row['impressions'])
        expanded_text = row['impressions']
        for abrv in doc._.abbreviations:
            expanded_text = expanded_text.replace(str(abrv), str(abrv._.long_form))

        # 2. LLM Triplet Extraction
        data = extract_with_retry(expanded_text, sys_instruct)

        if data:
            for tri in data.get('triplets', []):
                # 3. Grounding strings to UMLS
                subj_name, subj_cui = get_grounded_node(tri['subject'])
                obj_name, obj_cui = get_grounded_node(tri['object'])

                G.add_edge(
                    subj_name, obj_name,
                    relation=tri['predicate'],
                    cui_pair=(subj_cui, obj_cui),
                    sample_id=row['impression_id']
                )

        # Regular small delay to prevent immediate 429
        time.sleep(12)

    return G

# Start the pilot
kg_final = run_final_pilot('impressions_20250611.tsv', limit=100)

!pip install groq
!pip install python-dotenv


import pandas as pd
from groq import Groq
import spacy
import scispacy
from scispacy.abbreviation import AbbreviationDetector
from scispacy.linking import EntityLinker
import networkx as nx
import json
import time


import os
from dotenv import load_dotenv

# This looks for the .env file in the same folder and loads the variables
load_dotenv()

# Now fetch the key securely
api_key = os.getenv("GROQ_API_KEY")

if not api_key:
    raise ValueError("Groq API Key not found! Check your .env file.")

# --- INITIALIZATION ---
client = Groq(api_key)

# Load ScispaCy for local normalization and linking
nlp = spacy.load("en_core_sci_sm")
nlp.add_pipe("abbreviation_detector")    # PE / Pulmonary Embolism
nlp.add_pipe("scispacy_linker", config={"resolve_abbreviations": True, "linker_name": "umls"})
linker = nlp.get_pipe("scispacy_linker")

def get_grounded_node(text):
    """Link text to UMLS CUI to merge synonymous nodes."""
    doc = nlp(text)
    if doc.ents and doc.ents[0]._.kb_ents:
        cui = doc.ents[0]._.kb_ents[0][0]
        # Return canonical name from UMLS for consistent graph nodes
        return linker.kb.cui_to_entity[cui].canonical_name, cui
    return text, "Unknown"

def extract_with_groq(content, sys_instruct):
    """Uses Groq's JSON mode for reliable triplet extraction."""
    try:
        chat_completion = client.chat.completions.create(
            # Using Llama 3.3 70B for high-quality medical reasoning
            model="llama-3.3-70b-versatile",
            messages=[
                {"role": "system", "content": sys_instruct},
                {"role": "user", "content": f"Extract triplets from this text: {content}"}
            ],
            # JSON Mode ensures the output is a valid JSON object
            response_format={"type": "json_object"}
        )
        return json.loads(chat_completion.choices[0].message.content)
    except Exception as e:
        print(f"âŒ Groq Error: {e}")
        return None

# --- MAIN EXECUTION LOOP ---
def run_final_pilot(file_path, limit=100):
    df = pd.read_csv(file_path, sep='\t').head(limit)
    G = nx.MultiDiGraph()

    # Strict prompt for clinical triplets
    sys_instruct = (
        "You are a medical data architect. Extract knowledge triplets as JSON. "
        "Structure: {'triplets': [{'subject': 'Anatomy', 'predicate': 'Relation', 'object': 'Pathology/Assertion'}]} "
        "Use relations: located_in, manifested_as, associated_with."
    )

    for _, row in df.iterrows():
        print(f"Processing ID: {row['impression_id']}")

        # 1. Local Abbreviation Expansion (ScispaCy)
        doc = nlp(row['impressions'])
        expanded_text = row['impressions']
        for abrv in doc._.abbreviations:
            expanded_text = expanded_text.replace(str(abrv), str(abrv._.long_form))

        # 2. LLM Triplet Extraction (Groq)
        data = extract_with_groq(expanded_text, sys_instruct)

        if data:
            for tri in data.get('triplets', []):
                # 3. Grounding strings to UMLS (ScispaCy Linker)
                subj_name, subj_cui = get_grounded_node(tri['subject'])
                obj_name, obj_cui = get_grounded_node(tri['object'])

                G.add_edge(
                    subj_name, obj_name,
                    relation=tri['predicate'],
                    cui_pair=(subj_cui, obj_cui),
                    sample_id=row['impression_id']
                )

        # Groq is much faster than Gemini, but a small delay helps stay in free limits
        time.sleep(2)

    return G

# Start the pilot
kg_final = run_final_pilot('impressions_20250611.tsv', limit=100)

from pyvis.network import Network
import networkx as nx

def visualize_and_save_kg(G, output_file="PE_Reasoning_KG.html"):
    """
    Converts a NetworkX MultiDiGraph into an interactive HTML visualization.
    """
    # 1. Initialize the Pyvis Network
    # We use directed=True to show the flow of reasoning (Subject -> Object)
    net = Network(height="800px", width="100%", bgcolor="#222222", font_color="white", directed=True)

    # 2. Add Nodes and Edges from NetworkX
    for node_id in G.nodes:
        # Heuristic color mapping for the pilot
        # You can refine this by adding a 'type' attribute during graph construction
        color = "#97c2fc" # Default Blue
        label = str(node_id)

        if any(x in label.lower() for x in ["artery", "lobe", "vein", "lung", "heart"]):
            color = "#90EE90"  # Light Green for Anatomy
        elif any(x in label.lower() for x in ["defect", "clot", "embol", "effusion", "mass"]):
            color = "#FFA07A"  # Light Salmon for Pathology
        elif any(x in label.lower() for x in ["present", "absent", "possible", "none", "evidence"]):
            color = "#F0E68C"  # Khaki for Assertions

        net.add_node(node_id, label=label, color=color, title=f"Concept: {label}")

    for source, target, data in G.edges(data=True):
        # Create a descriptive label for the edge
        edge_label = data.get('relation', 'associated_with')
        sample_info = f"Sample ID: {data.get('sample_id', 'N/A')}"

        net.add_edge(source, target, label=edge_label, title=sample_info)

    # 3. Configure Physics (Crucial for 100-sample clusters)
    net.toggle_physics(True)
    net.set_options("""
    var options = {
      "physics": {
        "forceAtlas2Based": {
          "gravitationalConstant": -100,
          "centralGravity": 0.01,
          "springLength": 100,
          "springConstant": 0.08
        },
        "maxVelocity": 50,
        "solver": "forceAtlas2Based",
        "timestep": 0.35,
        "stabilization": { "iterations": 150 }
      }
    }
    """)

    # 4. Save and Export
    net.write_html(output_file)
    print(f"âœ… Knowledge Graph saved to: {output_file}")

    # --- Fix for GEXF serialization: Convert tuple to string ---
    # Create a copy to modify attributes without altering the original graph during iteration
    G_copy = G.copy()
    for u, v, key, data in G_copy.edges(keys=True, data=True):
        if 'cui_pair' in data and isinstance(data['cui_pair'], tuple):
            data['cui_pair'] = str(data['cui_pair'])
    # --- End of fix ---

    # Optional: Save as GEXF for professional tools like Gephi
    nx.write_gexf(G_copy, "pe_pilot_data.gexf")
    print("âœ… Structural data saved to: pe_pilot_data.gexf")

# --- Usage ---
visualize_and_save_kg(kg_final)

def export_triplets_to_csv(G, output_file="extracted_triplets_final.csv"):
    triplet_list = []

    # Iterate through all edges in the MultiDiGraph
    for u, v, data in G.edges(data=True):
        triplet_list.append({
            "impression_id": data.get("sample_id"),
            "subject": u,
            "predicate": data.get("relation"),
            "object": v,
            "subject_cui": data.get("cui_pair", (None, None))[0],
            "object_cui": data.get("cui_pair", (None, None))[1]
        })

    df_triplets = pd.DataFrame(triplet_list)
    df_triplets.to_csv(output_file, index=False)
    print(f"âœ… Extracted {len(df_triplets)} triplets to {output_file}")
    return df_triplets

# Usage
df_all_triplets = export_triplets_to_csv(kg_final)

def analyze_kg_insights(G):
    print("--- KG PILOT INSIGHTS ---")

    # 1. Connectivity Stats
    print(f"Total Unique Entities (Nodes): {G.number_of_nodes()}")
    print(f"Total Clinical Relations (Edges): {G.number_of_edges()}")

    # 2. Top Pathologies (Centrality)
    # Finds which findings are most frequently mentioned across the pilot
    degree_dict = dict(G.degree())
    top_entities = sorted(degree_dict.items(), key=lambda item: item[1], reverse=True)[:5]
    print("\nðŸ”¥ Top 5 Most Frequent Entities:")
    for entity, count in top_entities:
        print(f" - {entity}: mentioned {count} times")

    # 3. PE Reasoning specific: Find all "Present" assertions
    positive_cases = [u for u, v, d in G.edges(data=True) if v.lower() == 'present']
    print(f"\nâœ… Total Positive Findings Extracted: {len(positive_cases)}")

    # 4. Anatomy Distribution
    # Find which arteries are most commonly linked to a 'manifested_as' relation
    anatomy_counts = {}
    for u, v, d in G.edges(data=True):
        if d.get('relation') == 'located_in':
            anatomy_counts[v] = anatomy_counts.get(v, 0) + 1

    sorted_anatomy = sorted(anatomy_counts.items(), key=lambda x: x[1], reverse=True)
    print("\nðŸ“ Top Clot Locations (Anatomy):")
    for loc, count in sorted_anatomy[:5]:
        print(f" - {loc}: {count} occurrences")

# Usage
analyze_kg_insights(kg_final)

import pandas as pd
import networkx as nx

def analyze_and_export_pilot(G, output_csv="pe_triplets_pilot_100.csv"):
    # --- Part A: Extraction to CSV ---
    triplet_records = []
    for u, v, d in G.edges(data=True):
        triplet_records.append({
            "Impression_ID": d.get('sample_id'),
            "Subject": u,
            "Predicate": d.get('relation'),
            "Object": v,
            "CUI_Pair": d.get('cui_pair')
        })

    df_triplets = pd.DataFrame(triplet_records)
    df_triplets.to_csv(output_csv, index=False)

    # --- Part B: Clinical Reasoning Analysis ---
    print("ðŸ“Š --- PILOT DATA ANALYSIS --- ðŸ“Š")

    # 1. Prevalence of PE Status
    # This checks how many unique impressions were categorized as Present vs Absent
    status_counts = {"Present": 0, "Absent": 0, "Possible": 0}
    for u, v, d in G.edges(data=True):
        v_low = v.lower()
        if "present" in v_low: status_counts["Present"] += 1
        elif "absent" in v_low or "no evidence" in v_low: status_counts["Absent"] += 1
        elif "possible" in v_low or "rule out" in v_low: status_counts["Possible"] += 1

    print(f"\nâœ… Diagnostic Assertions Found:")
    for status, count in status_counts.items():
        print(f" - {status}: {count} triplets")

    # 2. Most Affected Anatomy (The Clot Map)
    clot_locations = {}
    for u, v, d in G.edges(data=True):
        if d.get('relation') == 'located_in':
            clot_locations[v] = clot_locations.get(v, 0) + 1

    print(f"\nðŸ“ Top 3 Anatomical Hotspots:")
    sorted_locs = sorted(clot_locations.items(), key=lambda x: x[1], reverse=True)[:3]
    for loc, count in sorted_locs:
        print(f" - {loc}: {count} occurrences")

    return df_triplets

# Execution
df_pilot_results = analyze_and_export_pilot(kg_final)

def generate_reasoning_audit(G, sample_id):
    """Retrieves all logic chains for a specific patient ID."""
    print(f"\nðŸ” Reasoning Audit for Sample {sample_id}:")
    chains = [f"{u} --[{d['relation']}]--> {v}" for u, v, d in G.edges(data=True) if d.get('sample_id') == sample_id]

    if not chains:
        print("No triplets found for this ID.")
    for c in chains:
        print(f"  â›“ï¸ {c}")

# Example use case for Sample 11561 (which has multiple emboli)
generate_reasoning_audit(kg_final, 11561)

def analyze_and_export_pilot_v2(G):
    print("ðŸ“Š --- REVISED PILOT DATA ANALYSIS --- ðŸ“Š")

    # Keywords that indicate a positive finding
    positive_keywords = ['present', 'detected', 'visible', 'confirmed', 'seen', 'acute', 'evidence']
    negative_keywords = ['absent', 'none', 'no', 'negative', 'normal', 'cleared']

    status_counts = {"Present": 0, "Absent": 0, "Possible": 0}

    for u, v, d in G.edges(data=True):
        # Combine Subject, Predicate, and Object into one string to search
        full_triplet_text = f"{u} {d.get('relation', '')} {v}".lower()

        # Priority 1: Check for Negation first (to avoid "No PE" being counted as Present)
        if any(neg in full_triplet_text for neg in negative_keywords):
            status_counts["Absent"] += 1
        # Priority 2: Check for Positive findings
        elif any(pos in full_triplet_text for pos in positive_keywords):
            status_counts["Present"] += 1
        # Priority 3: Check for Uncertainty
        elif "possible" in full_triplet_text or "evaluat" in full_triplet_text:
            status_counts["Possible"] += 1

    print(f"\nâœ… Diagnostic Assertions (Fuzzy Match):")
    for status, count in status_counts.items():
        print(f" - {status}: {count} triplets")

    return status_counts

    df_pilot_results = analyze_and_export_pilot_v2(kg_final)
    print(df_pilot_results)

"""SubGraph of Few Examples from reports :"""

import networkx as nx
import pandas as pd
from pyvis.network import Network

# 1. Load the existing graph from your downloaded file
# Ensure 'pe_pilot_data.gexf' is in your current working directory
G_loaded = nx.read_gexf("pe_pilot_data.gexf")

def get_subgraph_from_file(G, num_samples=6):
    # Get unique sample IDs from the edge attributes
    # In GEXF, attributes are stored in the edge data dictionary
    all_ids = list(set(d.get('sample_id') for u, v, d in G.edges(data=True)))
    target_ids = all_ids[:num_samples]

    print(f"Subsetting for Sample IDs: {target_ids}")

    # Create the smaller graph
    sub_G = nx.MultiDiGraph()
    for u, v, d in G.edges(data=True):
        if d.get('sample_id') in target_ids:
            sub_G.add_edge(u, v, **d)

    return sub_G

# Generate the 6-sample graph
kg_subset = get_subgraph_from_file(G_loaded, 3)

def visualize_subset(sub_G, output_file="debug_subset_view2.html"):
    net = Network(height="750px", width="100%", bgcolor="#222222", font_color="white", directed=True)
    net.from_nx(sub_G)

    # Simple color coding for clarity
    for node in net.nodes:
        label = node['id'].lower()
        if any(x in label for x in ["artery", "lobe", "lung"]):
            node['color'] = "#90EE90" # Anatomy
        elif any(x in label for x in ["present", "absent", "no evidence"]):
            node['color'] = "#F0E68C" # Assertion
        else:
            node['color'] = "#FFA07A" # Pathology

    net.toggle_physics(True)
    net.write_html(output_file)
    print(f"âœ… Subset visualization saved as: {output_file}")

visualize_subset(kg_subset)

"""Alignment of report and data extracted in graph"""

import pandas as pd
import networkx as nx
from IPython.display import display, HTML

# 1. Load the data
G = nx.read_gexf("pe_pilot_data.gexf")
df_orig = pd.read_csv("impressions_20250611.tsv", sep='\t')

def highlight_alignment(df_text, graph, num_samples=5):
    alignment_data = []

    # Get unique sample IDs from graph
    graph_ids = list(set(d.get('sample_id') for u, v, d in graph.edges(data=True)))

    for sid in graph_ids[:num_samples]:
        # Get original text
        original_text = df_text[df_text['impression_id'] == sid]['impressions'].values[0]

        # Get all triplets for this ID
        triplets = [f"({u}) --[{d['relation']}]--> ({v})" for u, v, d in graph.edges(data=True) if d.get('sample_id') == sid]
        entities = list(set([u for u, v, d in graph.edges(data=True) if d.get('sample_id') == sid] +
                            [v for u, v, d in graph.edges(data=True) if d.get('sample_id') == sid]))

        # Simple Highlighter: Wrap entities in colored spans
        highlighted_text = original_text
        for ent in entities:
            # We use a case-insensitive replace to highlight the entities found in the graph
            highlighted_text = highlighted_text.replace(ent, f'<span style="background-color: #ffff00; font-weight: bold;">{ent}</span>')

        alignment_data.append({
            "Sample ID": sid,
            "Highlighted Original Impression": highlighted_text,
            "Extracted Graph Triplets": "<br>".join(triplets)
        })

    # Display as a clean HTML table
    df_viz = pd.DataFrame(alignment_data)
    display(HTML(df_viz.to_html(escape=False, index=False)))

# Run the alignment
highlight_alignment(df_orig, G, num_samples=10)

import networkx as nx
import pandas as pd

# 1. Load the graph from your GEXF file
G = nx.read_gexf("pe_pilot_data.gexf")

def export_subgraph_csv(full_graph, num_samples=6, output_name="subgraph_triplets.csv"):
    # 2. Identify the first 6 unique sample IDs in the graph
    # GEXF attributes are accessed via the edge data dictionary
    sample_ids = []
    for u, v, d in full_graph.edges(data=True):
        sid = d.get('sample_id')
        if sid and sid not in sample_ids:
            sample_ids.append(sid)
        if len(sample_ids) >= num_samples:
            break

    # 3. Filter edges belonging to these samples
    subgraph_data = []
    for u, v, d in full_graph.edges(data=True):
        if d.get('sample_id') in sample_ids:
            subgraph_data.append({
                "sample_id": d.get('sample_id'),
                "subject": u,
                "predicate": d.get('relation'),
                "object": v,
                "cui_pair": d.get('cui_pair')
            })

    # 4. Create DataFrame and Save
    df_sub = pd.DataFrame(subgraph_data)
    df_sub.to_csv(output_name, index=False)
    print(f"âœ… Exported {len(df_sub)} triplets from {len(sample_ids)} samples to {output_name}")
    return df_sub

# Run the export
sub_df = export_subgraph_csv(G, num_samples=6)

