# -*- coding: utf-8 -*-
"""vision_backbone_MedViT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_dnuaiA36YXCw0yE2x7wqcjz4c_oHERf
"""

!pip install monai tqdm

"""### 1. Install Libraries"""

!pip install timm

"""### 2. Define the TriWindowStackD Custom Transform"""

import torch
import nibabel as nib
import numpy as np
from monai.transforms import (
    Compose, LoadImaged, EnsureChannelFirstD,
    ScaleIntensityRanged, Resized, ToTensord
)
from monai.data import Dataset, DataLoader

class TriWindowStackD:
    """
    Custom MONAI transform to create the Lung, PE, and Mediastinum stack.
    """
    def __init__(self, keys):
        self.keys = keys

    def __call__(self, data):
        d = dict(data)
        for key in self.keys:
            img = d[key]
            # 1. Lung Window (W:1500, L:-600) -> Range [-1350, 150]
            lung = torch.clamp((img - (-600 - 1500//2)) / 1500, 0, 1)
            # 2. PE Window (W:700, L:100) -> Range [-250, 450]
            pe = torch.clamp((img - (100 - 700//2)) / 700, 0, 1)
            # 3. Mediastinum Window (W:400, L:40) -> Range [-160, 240]
            mediastinum = torch.clamp((img - (40 - 400//2)) / 400, 0, 1)

            # Stack into 3 channels: (3, D, H, W)
            d[key] = torch.cat([lung, pe, mediastinum], dim=0)
        return d

"""### 3. Define the TWGR_MedViT Model"""

import torch
import torch.nn as nn
import timm

# Assuming you have the MedViT repository or library installed
# pip install timm (often used as a base for MedViT implementations)
# from MedViT import MedViT_small # Representative import

class TWGR_MedViT(nn.Module):
    def __init__(self, llm_dim=4096):
        super().__init__()
        # Load MedViT pretrained on MedMNIST/ImageNet using timm
        self.base_model = timm.create_model('vit_tiny_patch16_224', pretrained=True) # Placeholder for MedViT_small-like model in timm
        # Note: 'vit_tiny_patch16_224' is a common small ViT model in timm.
        # You may need to find the specific MedViT equivalent if available in timm,
        # or replace with your custom MedViT implementation if not from timm.

        # Remove the classification head if it exists and is not needed
        if hasattr(self.base_model, 'head'):
            self.encoder = nn.Sequential(*list(self.base_model.children())[:-1])
        else:
            self.encoder = self.base_model # Use the full model if no head to remove explicitly

        # MedViT_small hidden size is usually 512 or 768.
        # For 'vit_tiny_patch16_224' the feature dimension is 192 (embed_dim). Adjust accordingly.
        # self.projector = nn.Linear(512, llm_dim)
        self.projector = nn.Linear(self.base_model.num_features, llm_dim)

    def forward(self, x):
        """
        Input x: (Batch, 3, D, H, W)
        Process: We treat D (depth) as part of the batch to use 2D hybrid blocks
        """
        b, c, d, h, w = x.shape
        # Permute to process each depth slice through the hybrid blocks
        # (B*D, C, H, W)
        x = x.permute(0, 2, 1, 3, 4).reshape(-1, c, h, w)

        features = self.encoder(x) # Shape: (B*D, feature_dim, 1, 1) or (B*D, feature_dim) after pooling
        if features.ndim == 4: # If output is (B*D, feature_dim, 1, 1)
            features = features.squeeze(-1).squeeze(-1)

        features = features.view(b, d, -1) # Restore sequence: (B, D, feature_dim)

        return self.projector(features)

"""### 4. Setup Data Loader for Model Inspection (Old Definition)"""

def get_inspect_loader(file_list, labels, batch_size=2):
    # Data list format: [{"image": "path/PEc3791b.nii", "label": 1}, ...]
    data_dicts = [{"image": f, "label": l} for f, l in zip(file_list, labels)]

    transforms = Compose([
        LoadImaged(keys=["image"]),
        EnsureChannelFirstD(keys=["image"]),
        # Standardize resolution for MedViT (e.g., 128x128 per slice)
        Resized(keys=["image"], spatial_size=(128, 128, 128)),
        TriWindowStackD(keys=["image"]),
        ToTensord(keys=["image", "label"])
    ])

    ds = Dataset(data=data_dicts, transform=transforms)
    loader = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=4)
    return loader

"""### 6. Main Data Pipeline and Model Inference"""

import torch
from torch.utils.data import DataLoader
# Assuming the classes we drafted (TriWindowStackD, TWGR_MedViT) are in 'model_defs.py'
# from model_defs import TriWindowStackD, TWGR_MedViT
from monai.transforms import Compose, LoadImaged, EnsureChannelFirstD, Resized, ToTensord
from monai.data import Dataset

# 1. SETUP PATHS (For your 100-sample pilot)
# Replace these with your actual local paths to the INSPECT .nii files
file_paths = ["/content/PEc3791b.nii.gz"] # example list, changed to .nii.gz
labels = [1] # 1 for PE Detected, 0 for Negative

# 2. CONSTRUCT DATA PIPELINE
data_dicts = [{"image": f, "label": l} for f, l in zip(file_paths, labels)]

# --- Temporarily inspect original dimensions ---
temp_transforms = Compose([
    LoadImaged(keys=["image"]),
    EnsureChannelFirstD(keys=["image"])
])
temp_dataset = Dataset(data=data_dicts, transform=temp_transforms)
# Load the first item to get its original shape
first_scan_data = temp_dataset[0]["image"]
print(f"Original scan dimensions (Channels, Depth, Height, Width) before resizing: {first_scan_data.shape}")
# ------------------------------------------------

transforms = Compose([
    LoadImaged(keys=["image"]),
    EnsureChannelFirstD(keys=["image"]),
    Resized(keys=["image"], spatial_size=(224, 224, 224)), # Standardize Voxel Grid to match model input
    TriWindowStackD(keys=["image"]),
    ToTensord(keys=["image", "label"])
])

# Use MONAI Dataset for efficient metadata handling
dataset = Dataset(data=data_dicts, transform=transforms)
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

# 3. INITIALIZE BACKBONE
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = TWGR_MedViT(llm_dim=4096).to(device)

# 4. RUN INFERENCE LOOP
model.eval()
with torch.no_grad():
    for batch in dataloader:
        # Move tri-window volume to GPU: (Batch, 3, 128, 128, 128)
        inputs = batch["image"].to(device)

        # Forward Pass: Extract visual tokens projected to LLM space
        # Output Shape: (Batch, Tokens, 4096)
        visual_embeddings = model(inputs)

        print(f"Successfully generated {visual_embeddings.shape[1]} visual tokens per scan.")
        print(f"Embedding Shape: {visual_embeddings.shape}")

        # NEXT STEP: These embeddings can now be concatenated with your KG text tokens
        # e.g., full_prompt = torch.cat([visual_embeddings, kg_text_embeddings], dim=1)
        break

"""### 7. Display Generated Visual Embeddings"""

visual_embeddings

"""### 8. Visualize Processed (Tri-Windowed and Resized) Images"""

import matplotlib.pyplot as plt

# Get one batch from the dataloader
# Make sure to re-initialize the dataloader if it has already been iterated through
# Or, if running this immediately after the previous cell, 'batch' should still be available.

# For robustness, let's re-create a DataLoader to ensure we can get a fresh batch
dataset = Dataset(data=data_dicts, transform=transforms)
dataloader_viz = DataLoader(dataset, batch_size=1, shuffle=False) # Use batch_size=1 for easy visualization

# Get the first item from the dataloader_viz
# Next(iter(dataloader_viz)) will fetch the next batch
batch = next(iter(dataloader_viz))
inputs = batch["image"].cpu().numpy() # Move to CPU and convert to numpy for plotting

# Take the first image from the batch (batch_size is 1 here)
image_3d = inputs[0] # Shape: (C, D, H, W) e.g., (3, 224, 224, 224)

# Define slices to visualize (e.g., start, middle, end of depth)
depth_dim = image_3d.shape[1] # D dimension
slices_to_show = [depth_dim // 4, depth_dim // 2, 3 * depth_dim // 4]

window_names = ["Lung Window", "PE Window", "Mediastinum Window"]

fig, axes = plt.subplots(len(slices_to_show), 3, figsize=(15, 5 * len(slices_to_show)))
fig.suptitle("Visualizing Tri-Windowed Slices", fontsize=16, y=1.02)

for i, slice_idx in enumerate(slices_to_show):
    for j in range(3): # Iterate through the 3 channels (Lung, PE, Mediastinum)
        ax = axes[i, j]
        # Display the 2D slice
        ax.imshow(image_3d[j, slice_idx, :, :], cmap='gray')
        ax.set_title(f"{window_names[j]} - Slice {slice_idx}")
        ax.axis('off') # Hide axes ticks

plt.tight_layout()
plt.show()

"""### 9. Visualize Raw Images (Before Preprocessing)"""

import matplotlib.pyplot as plt

# For robustness, let's re-create a DataLoader to ensure we can get a fresh batch
dataset = Dataset(data=data_dicts, transform=transforms)
dataloader_viz = DataLoader(dataset, batch_size=1, shuffle=False) # Use batch_size=1 for easy visualization

# Get the first item from the dataloader_viz
batch = next(iter(dataloader_viz))
inputs = batch["image"].cpu().numpy() # Move to CPU and convert to numpy for plotting

# Take the first image from the batch (batch_size is 1 here)
image_3d = inputs[0] # Shape: (C, D, H, W) e.g., (3, 224, 224, 224)

# Define slices to visualize (e.g., start, middle, end of depth)
depth_dim = image_3d.shape[1] # D dimension is 224

# Calculate proportionally equivalent slices based on user's request for original 94, 188, 282 from 377 depth
# Original depth was 377. Resized depth is 224.
original_depth_for_ratio = 377 # From previous `Original raw image shape: (512, 512, 377)`
requested_raw_slices = [94, 188, 282]
slices_to_show = [round(s / original_depth_for_ratio * depth_dim) for s in requested_raw_slices]

window_names = ["Lung Window", "PE Window", "Mediastinum Window"]

fig, axes = plt.subplots(len(slices_to_show), 3, figsize=(15, 5 * len(slices_to_show)))
fig.suptitle("Visualizing Tri-Windowed Slices (Proportional to Original Raw Image)", fontsize=16, y=1.02)

for i, slice_idx in enumerate(slices_to_show):
    for j in range(3): # Iterate through the 3 channels (Lung, PE, Mediastinum)
        ax = axes[i, j]
        # Display the 2D slice
        ax.imshow(image_3d[j, slice_idx, :, :], cmap='gray')
        ax.set_title(f"{window_names[j]} - Resized Slice {slice_idx} (Orig: {requested_raw_slices[i]}) ")
        ax.axis('off') # Hide axes ticks

plt.tight_layout()
plt.show()

import nibabel as nib
import matplotlib.pyplot as plt
import numpy as np

# Assuming file_paths is already defined from previous cells
# For robustness, let's explicitly get the first file path
first_file_path = file_paths[0]

# Load the original NIfTI image
img = nib.load(first_file_path)
data = img.get_fdata() # Get the image data as a NumPy array

print(f"Original raw image shape: {data.shape}") # Should be (Height, Width, Depth) based on user clarification

# Define slices to visualize - corrected to use the true depth dimension
depth_dim_raw = data.shape[2] # The depth dimension is the third one (377 slices)

slices_to_show_raw = [depth_dim_raw // 4, depth_dim_raw // 2, 3 * depth_dim_raw // 4]

fig, axes = plt.subplots(1, len(slices_to_show_raw), figsize=(15, 5))
fig.suptitle(f"Visualizing Raw Slices from {first_file_path} (Depth Axis)", fontsize=16)

for i, slice_idx in enumerate(slices_to_show_raw):
    ax = axes[i]
    # Display the 2D slice, now correctly taking slices along the third dimension (depth)
    ax.imshow(data[:, :, slice_idx], cmap='gray')
    ax.set_title(f"Raw Slice {slice_idx}")
    ax.axis('off')

plt.tight_layout()
plt.show()

"""This visualization shows selected slices directly from the raw NIfTI image data, before any MONAI transforms like channel-first conversion, resizing, or tri-window applications. This allows you to see the original pixel intensities and spatial dimensions."""

